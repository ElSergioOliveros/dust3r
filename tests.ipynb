{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876e95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, cannot find cuda-compiled version of RoPE2D, using a slow pytorch version instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soliverosb/3dStuff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/soliverosb/dust3r/dust3r/cloud_opt/base_opt.py:275: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n"
     ]
    }
   ],
   "source": [
    "from dust3r.inference import inference\n",
    "from dust3r.model import AsymmetricCroCo3DStereo\n",
    "from dust3r.utils.image import load_images_from_PIL\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n",
    "\n",
    "import PIL.Image \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1936dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/soliverosb/OWL/src/external_repositories/data/apple/frame_annotations.js', 'r') as file:\n",
    "    annots = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5334fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "annots.sort(key=lambda x: x['frame_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "177fb258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence_name': '540_79043_153212',\n",
       " 'frame_number': 1,\n",
       " 'frame_timestamp': 0.0,\n",
       " 'image': {'path': 'apple/540_79043_153212/images/frame000001.jpg',\n",
       "  'size': [900, 2000]},\n",
       " 'depth': {'path': 'apple/540_79043_153212/depths/frame000001.jpg.geometric.png',\n",
       "  'scale_adjustment': 1.0,\n",
       "  'mask_path': 'apple/540_79043_153212/depth_masks/frame000001.png'},\n",
       " 'mask': {'path': 'apple/540_79043_153212/masks/frame000001.png',\n",
       "  'mass': 222916.0},\n",
       " 'viewpoint': {'R': [[-0.9970602989196777,\n",
       "    -0.07412637025117874,\n",
       "    0.019390851259231567],\n",
       "   [0.07354791462421417, -0.9968695640563965, -0.02901434898376465],\n",
       "   [0.021480876952409744, -0.02750289812684059, 0.9993909001350403]],\n",
       "  'T': [-0.6326847672462463, 1.212049126625061, 14.048179626464844],\n",
       "  'focal_length': [3.922766923904419, 3.923976182937622],\n",
       "  'principal_point': [-0.0, -0.0011111111380159855],\n",
       "  'intrinsics_format': 'ndc_isotropic'},\n",
       " 'meta': {'frame_type': 'test_known',\n",
       "  'frame_splits': ['singlesequence_apple_test_0_known'],\n",
       "  'eval_batch_maps': []},\n",
       " 'camera_name': None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annots[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22922232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/soliverosb/dust3r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68f713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " (Found 30 images)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "schedule = 'cosine'\n",
    "lr = 0.01\n",
    "niter = 300\n",
    "\n",
    "model_name = \"naver/DUSt3R_ViTLarge_BaseDecoder_512_dpt\"\n",
    "\n",
    "model = AsymmetricCroCo3DStereo.from_pretrained(model_name).to(device)\n",
    "\n",
    "imageDf = pd.read_csv(\"/home/soliverosb/dataFor3D/carScene.csv\")\n",
    "n = 30\n",
    "PILImages = [ PIL.Image.open(imageDf['paths'].iloc[i]) for i in range(n) ]\n",
    "transformationMatrices = torch.tensor([ast.literal_eval(imageDf['rotation_matrices'].iloc[i]) for i in range(n)])\n",
    "\n",
    "images = load_images_from_PIL(PILImages, size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ee33a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Inference with model on 870 image pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/55 [00:00<?, ?it/s]/home/soliverosb/dust3r/dust3r/inference.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=bool(use_amp)):\n",
      "/home/soliverosb/dust3r/dust3r/model.py:205: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/soliverosb/dust3r/dust3r/inference.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "100%|██████████| 55/55 [01:30<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " init edge (12*,23*) score=np.float64(16.18464469909668)\n",
      " init edge (23,24*) score=np.float64(14.476905822753906)\n",
      " init edge (23,10*) score=np.float64(13.846390724182129)\n",
      " init edge (14*,24) score=np.float64(13.77676773071289)\n",
      " init edge (23,25*) score=np.float64(13.392385482788086)\n",
      " init edge (21*,24) score=np.float64(13.24314022064209)\n",
      " init edge (13*,24) score=np.float64(13.002606391906738)\n",
      " init edge (28*,25) score=np.float64(12.237759590148926)\n",
      " init edge (22*,12) score=np.float64(11.685663223266602)\n",
      " init edge (22,8*) score=np.float64(10.564742088317871)\n",
      " init edge (28,26*) score=np.float64(10.49522876739502)\n",
      " init edge (7*,24) score=np.float64(9.887109756469727)\n",
      " init edge (20*,23) score=np.float64(9.643139839172363)\n",
      " init edge (13,11*) score=np.float64(9.519482612609863)\n",
      " init edge (1*,12) score=np.float64(8.512975692749023)\n",
      " init edge (2*,23) score=np.float64(8.185733795166016)\n",
      " init edge (12,16*) score=np.float64(6.656954288482666)\n",
      " init edge (13,9*) score=np.float64(6.651501655578613)\n",
      " init edge (7,3*) score=np.float64(5.544500350952148)\n",
      " init edge (23,4*) score=np.float64(5.057535648345947)\n",
      " init edge (22,29*) score=np.float64(4.511457443237305)\n",
      " init edge (19*,20) score=np.float64(4.027135372161865)\n",
      " init edge (24,15*) score=np.float64(3.607835292816162)\n",
      " init edge (0*,28) score=np.float64(2.539544105529785)\n",
      " init edge (17*,25) score=np.float64(14.143184661865234)\n",
      " init edge (6*,17) score=np.float64(13.255047798156738)\n",
      " init edge (11,27*) score=np.float64(12.154653549194336)\n",
      " init edge (5*,27) score=np.float64(6.627800464630127)\n",
      " init edge (18*,27) score=np.float64(5.944089889526367)\n",
      " init loss = 0.00738320779055357\n",
      "Global alignement - optimizing for:\n",
      "['pw_poses', 'im_depthmaps', 'im_poses', 'im_focals']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:26<00:00,  2.05it/s, lr=1.27413e-06 loss=0.00431767]\n"
     ]
    }
   ],
   "source": [
    "pairs = make_pairs(images, scene_graph='complete', prefilter='cyc20', symmetrize=True)\n",
    "output = inference(pairs, model, device, batch_size=16)\n",
    "\n",
    "\n",
    "scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PointCloudOptimizer)\n",
    "loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)\n",
    "\n",
    "# retrieve useful values from scene:\n",
    "imgs = scene.imgs\n",
    "focals = scene.get_focals()\n",
    "poses = scene.get_im_poses().cpu().detach()\n",
    "pts3d = scene.get_pts3d()\n",
    "confidence_masks = scene.get_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95ebe1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReferenceFrameTransform(X, Y):\n",
    "    mu_x = torch.mean(X, dim=0)\n",
    "    mu_y = torch.mean(Y, dim=0)\n",
    "\n",
    "    var_x = torch.square(X - mu_x).sum(dim=1).mean()\n",
    "    var_y = torch.square(Y - mu_y).sum(dim=1).mean()\n",
    "    \n",
    "    cov_XY = torch.mm((Y - mu_y).T,(X - mu_x))/len(X)\n",
    "\n",
    "    U, D, Vh = torch.linalg.svd(cov_XY)\n",
    "\n",
    "    S = torch.eye(X.shape[1])\n",
    "\n",
    "    if X.shape[1]-1 < torch.linalg.matrix_rank(cov_XY):\n",
    "        if torch.linalg.det(cov_XY) < 0:\n",
    "            S[-1, -1] = -1\n",
    "    else: \n",
    "        if torch.linalg.det(U) * torch.linalg.det(Vh) < 0:\n",
    "            S[-1, -1] = -1\n",
    "    \n",
    "    c = 1/var_x*torch.trace(torch.diag(D)@S)\n",
    "    R = U @ S @ Vh\n",
    "    t = mu_y - c*R@mu_x\n",
    "\n",
    "    return R, t, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e3f1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllCombinations(batch):\n",
    "    indices_i = []\n",
    "    indices_j = []\n",
    "    batch_size = len(batch)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(i + 1, batch_size):\n",
    "            indices_i.append(i)\n",
    "            indices_j.append(j)\n",
    "\n",
    "    indices_i = torch.tensor(indices_i)\n",
    "    indices_j = torch.tensor(indices_j)\n",
    "\n",
    "    tensors_i = batch[indices_i]\n",
    "    tensors_j = batch[indices_j]\n",
    "\n",
    "    return tensors_i, tensors_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b6ec9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcRTA(positionPreds, positionGt, tau=15):\n",
    "\n",
    "    positionsPreds_i, positionsPreds_j = getAllCombinations(positionPreds)\n",
    "    positionsGt_i, positionsGt_j = getAllCombinations(positionGt)\n",
    "\n",
    "    positionDifferencesPreds = positionsPreds_j - positionsPreds_i\n",
    "    positionDifferencesGt = positionsGt_j - positionsGt_i\n",
    "\n",
    "    positionDifferencesPreds = torch.nn.functional.normalize(positionDifferencesPreds)\n",
    "    positionDifferencesGt = torch.nn.functional.normalize(positionDifferencesGt)\n",
    "\n",
    "    positionDotDifferences = torch.sum(positionDifferencesPreds*positionDifferencesGt, dim=1)\n",
    "    positionDotAngles = torch.rad2deg(torch.acos(positionDotDifferences))\n",
    "\n",
    "    RTA = torch.where(positionDotAngles < tau, 1.0, 0.0).mean()\n",
    "\n",
    "    return RTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96dcb2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcRRA(rotationPreds, rotationGt, tau=15):\n",
    "    rotationPreds_i, rotationPreds_j = getAllCombinations(rotationPreds)\n",
    "    rotationGt_i, rotationGt_j = getAllCombinations(rotationGt)\n",
    "\n",
    "    rotationPreds_ij = torch.matmul(rotationPreds_i, rotationPreds_j.transpose(1,2))\n",
    "    rotationGt_ij = torch.matmul(rotationGt_i, rotationGt_j.transpose(1,2))\n",
    "    \n",
    "    rotationGtPred_ij = torch.matmul(rotationGt_ij.transpose(1,2), rotationPreds_ij)\n",
    "\n",
    "    traces = torch.einsum('bii->b', rotationGtPred_ij)\n",
    "\n",
    "    rotationAngles_ij = torch.rad2deg(torch.acos((traces -1)/2))\n",
    "\n",
    "    RRA = torch.where(rotationAngles_ij < tau, 1.0, 0.0).mean()\n",
    "\n",
    "    return RRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2aae21b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9862), tensor(0.9977))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcError(HPreds, HGt):\n",
    "    positionPreds = HPreds[:, :3, 3]\n",
    "    positionGt = HGt[:, :3, 3]\n",
    "\n",
    "    rotationPreds = HPreds[:, :3, :3]\n",
    "    rotationGt = HGt[:, :3, :3]\n",
    "\n",
    "    \n",
    "    R, t, c = getReferenceFrameTransform(positionPreds, positionGt)\n",
    "\n",
    "    positionPredsAligned = c*torch.matmul(positionPreds, R.T) + t\n",
    "    rotationPredsAligned = torch.matmul(R, rotationPreds)\n",
    "\n",
    "    RTA = calcRTA(positionPredsAligned, positionGt)\n",
    "\n",
    "    RRA = calcRRA(rotationPredsAligned, rotationGt)\n",
    "    \n",
    "    return RTA, RRA\n",
    "    \n",
    "\n",
    "calcError(poses, transformationMatrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b460c734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9862), tensor(0.9931))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcError(poses, transformationMatrices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dStuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
