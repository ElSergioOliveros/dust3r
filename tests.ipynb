{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876e95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, cannot find cuda-compiled version of RoPE2D, using a slow pytorch version instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soliverosb/3dStuff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/soliverosb/dust3r/dust3r/cloud_opt/base_opt.py:275: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n"
     ]
    }
   ],
   "source": [
    "from dust3r.inference import inference\n",
    "from dust3r.model import AsymmetricCroCo3DStereo\n",
    "from dust3r.utils.image import load_images_from_PIL\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n",
    "from dust3r.demo import get_3D_model_from_scene\n",
    "import PIL.Image \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0e68f713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " - changing image with resolution 800x800 --> 512x384\n",
      " (Found 30 images)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "schedule = 'cosine'\n",
    "lr = 0.01\n",
    "niter = 300\n",
    "\n",
    "model_name = \"naver/DUSt3R_ViTLarge_BaseDecoder_512_dpt\"\n",
    "# you can put the path to a local checkpoint in model_name if needed\n",
    "model = AsymmetricCroCo3DStereo.from_pretrained(model_name).to(device)\n",
    "# load_images can take a list of images or a directory\n",
    "\n",
    "imageDf = pd.read_csv(\"/home/soliverosb/dataFor3D/carScene.csv\")\n",
    "n = 30\n",
    "PILImages = [ PIL.Image.open(imageDf['paths'].iloc[i]) for i in range(n) ]\n",
    "transformationMatrices = torch.tensor([ast.literal_eval(imageDf['rotation_matrices'].iloc[i]) for i in range(n)])\n",
    "\n",
    "images = load_images_from_PIL(PILImages, size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ee33a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Inference with model on 870 image pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/55 [00:00<?, ?it/s]/home/soliverosb/dust3r/dust3r/inference.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=bool(use_amp)):\n",
      "/home/soliverosb/dust3r/dust3r/model.py:205: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/soliverosb/dust3r/dust3r/inference.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "100%|██████████| 55/55 [01:30<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " init edge (12*,23*) score=np.float64(16.18464469909668)\n",
      " init edge (23,24*) score=np.float64(14.476905822753906)\n",
      " init edge (23,10*) score=np.float64(13.846390724182129)\n",
      " init edge (14*,24) score=np.float64(13.77676773071289)\n",
      " init edge (23,25*) score=np.float64(13.392385482788086)\n",
      " init edge (21*,24) score=np.float64(13.24314022064209)\n",
      " init edge (13*,24) score=np.float64(13.002606391906738)\n",
      " init edge (28*,25) score=np.float64(12.237759590148926)\n",
      " init edge (22*,12) score=np.float64(11.685663223266602)\n",
      " init edge (22,8*) score=np.float64(10.564742088317871)\n",
      " init edge (28,26*) score=np.float64(10.49522876739502)\n",
      " init edge (7*,24) score=np.float64(9.887109756469727)\n",
      " init edge (20*,23) score=np.float64(9.643139839172363)\n",
      " init edge (13,11*) score=np.float64(9.519482612609863)\n",
      " init edge (1*,12) score=np.float64(8.512975692749023)\n",
      " init edge (2*,23) score=np.float64(8.185733795166016)\n",
      " init edge (12,16*) score=np.float64(6.656954288482666)\n",
      " init edge (13,9*) score=np.float64(6.651501655578613)\n",
      " init edge (7,3*) score=np.float64(5.544500350952148)\n",
      " init edge (23,4*) score=np.float64(5.057535648345947)\n",
      " init edge (22,29*) score=np.float64(4.511457443237305)\n",
      " init edge (19*,20) score=np.float64(4.027135372161865)\n",
      " init edge (24,15*) score=np.float64(3.607835292816162)\n",
      " init edge (0*,28) score=np.float64(2.539544105529785)\n",
      " init edge (17*,25) score=np.float64(14.143184661865234)\n",
      " init edge (6*,17) score=np.float64(13.255047798156738)\n",
      " init edge (11,27*) score=np.float64(12.154653549194336)\n",
      " init edge (5*,27) score=np.float64(6.627800464630127)\n",
      " init edge (18*,27) score=np.float64(5.944089889526367)\n",
      " init loss = 0.00738320779055357\n",
      "Global alignement - optimizing for:\n",
      "['pw_poses', 'im_depthmaps', 'im_poses', 'im_focals']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:26<00:00,  2.05it/s, lr=1.27413e-06 loss=0.00431767]\n"
     ]
    }
   ],
   "source": [
    "pairs = make_pairs(images, scene_graph='complete', prefilter='cyc20', symmetrize=True)\n",
    "output = inference(pairs, model, device, batch_size=16)\n",
    "\n",
    "\n",
    "scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PointCloudOptimizer)\n",
    "loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)\n",
    "\n",
    "# retrieve useful values from scene:\n",
    "imgs = scene.imgs\n",
    "focals = scene.get_focals()\n",
    "poses = scene.get_im_poses().cpu().detach()\n",
    "pts3d = scene.get_pts3d()\n",
    "confidence_masks = scene.get_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95ebe1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReferenceFrameTransform(X, Y):\n",
    "    mu_x = torch.mean(X, dim=0)\n",
    "    mu_y = torch.mean(Y, dim=0)\n",
    "\n",
    "    var_x = torch.square(X - mu_x).sum(dim=1).mean()\n",
    "    var_y = torch.square(Y - mu_y).sum(dim=1).mean()\n",
    "    \n",
    "    cov_XY = torch.mm((Y - mu_y).T,(X - mu_x))/len(X)\n",
    "\n",
    "    U, D, Vh = torch.linalg.svd(cov_XY)\n",
    "\n",
    "    S = torch.eye(X.shape[1])\n",
    "\n",
    "    if X.shape[1]-1 < torch.linalg.matrix_rank(cov_XY):\n",
    "        if torch.linalg.det(cov_XY) < 0:\n",
    "            S[-1, -1] = -1\n",
    "    else: \n",
    "        if torch.linalg.det(U) * torch.linalg.det(Vh) < 0:\n",
    "            S[-1, -1] = -1\n",
    "    \n",
    "    c = 1/var_x*torch.trace(torch.diag(D)@S)\n",
    "    R = U @ S @ Vh\n",
    "    t = mu_y - c*R@mu_x\n",
    "\n",
    "    return R, t, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e3f1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllCombinations(batch):\n",
    "    indices_i = []\n",
    "    indices_j = []\n",
    "    batch_size = len(batch)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(i + 1, batch_size):\n",
    "            indices_i.append(i)\n",
    "            indices_j.append(j)\n",
    "\n",
    "    indices_i = torch.tensor(indices_i)\n",
    "    indices_j = torch.tensor(indices_j)\n",
    "\n",
    "    tensors_i = batch[indices_i]\n",
    "    tensors_j = batch[indices_j]\n",
    "\n",
    "    return tensors_i, tensors_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b6ec9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcRTA(positionPreds, positionGt, tau=15):\n",
    "\n",
    "    positionsPreds_i, positionsPreds_j = getAllCombinations(positionPreds)\n",
    "    positionsGt_i, positionsGt_j = getAllCombinations(positionGt)\n",
    "\n",
    "    positionDifferencesPreds = positionsPreds_j - positionsPreds_i\n",
    "    positionDifferencesGt = positionsGt_j - positionsGt_i\n",
    "\n",
    "    positionDifferencesPreds = torch.nn.functional.normalize(positionDifferencesPreds)\n",
    "    positionDifferencesGt = torch.nn.functional.normalize(positionDifferencesGt)\n",
    "\n",
    "    positionDotDifferences = torch.sum(positionDifferencesPreds*positionDifferencesGt, dim=1)\n",
    "    positionDotAngles = torch.rad2deg(torch.acos(positionDotDifferences))\n",
    "\n",
    "    RTA = torch.where(positionDotAngles < tau, 1.0, 0.0).mean()\n",
    "\n",
    "    return RTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96dcb2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcRRA(rotationPreds, rotationGt, tau=15):\n",
    "    rotationPreds_i, rotationPreds_j = getAllCombinations(rotationPreds)\n",
    "    rotationGt_i, rotationGt_j = getAllCombinations(rotationGt)\n",
    "\n",
    "    rotationPreds_ij = torch.matmul(rotationPreds_i, rotationPreds_j.transpose(1,2))\n",
    "    rotationGt_ij = torch.matmul(rotationGt_i, rotationGt_j.transpose(1,2))\n",
    "    \n",
    "    rotationGtPred_ij = torch.matmul(rotationGt_ij.transpose(1,2), rotationPreds_ij)\n",
    "\n",
    "    traces = torch.einsum('bii->b', rotationGtPred_ij)\n",
    "\n",
    "    rotationAngles_ij = torch.rad2deg(torch.acos((traces -1)/2))\n",
    "\n",
    "    RRA = torch.where(rotationAngles_ij < tau, 1.0, 0.0).mean()\n",
    "\n",
    "    return RRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2aae21b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9862), tensor(0.9977))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcError(HPreds, HGt):\n",
    "    positionPreds = HPreds[:, :3, 3]\n",
    "    positionGt = HGt[:, :3, 3]\n",
    "\n",
    "    rotationPreds = HPreds[:, :3, :3]\n",
    "    rotationGt = HGt[:, :3, :3]\n",
    "\n",
    "    \n",
    "    R, t, c = getReferenceFrameTransform(positionPreds, positionGt)\n",
    "\n",
    "    positionPredsAligned = c*torch.matmul(positionPreds, R.T) + t\n",
    "    rotationPredsAligned = torch.matmul(R, rotationPreds)\n",
    "\n",
    "    RTA = calcRTA(positionPredsAligned, positionGt)\n",
    "\n",
    "    RRA = calcRRA(rotationPredsAligned, rotationGt)\n",
    "    \n",
    "    return RTA, RRA\n",
    "    \n",
    "\n",
    "calcError(poses, transformationMatrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b460c734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9862), tensor(0.9931))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcError(poses, transformationMatrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5916690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5827f461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrices Rotación\n",
      "ángulo pred: tensor(24.9160) ángulo gt: 20.713814 ángulo relativo: 45.50898\n",
      "Vectores posición\n",
      "ángulo pred: 135.58289 ángulo gt: 168.53445 ángulo relativo: 35.174015\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(90.9673) ángulo gt: 100.10851 ángulo relativo: 169.3059\n",
      "Vectores posición\n",
      "ángulo pred: 136.33514 ángulo gt: 132.10344 ángulo relativo: 83.84451\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(131.7434) ángulo gt: 137.08923 ángulo relativo: 97.66106\n",
      "Vectores posición\n",
      "ángulo pred: 107.90105 ángulo gt: 128.11404 ángulo relativo: 35.532993\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(27.8890) ángulo gt: 27.12539 ángulo relativo: 53.66632\n",
      "Vectores posición\n",
      "ángulo pred: 136.20004 ángulo gt: 133.00377 ángulo relativo: 68.99313\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(159.8584) ángulo gt: 162.93022 ángulo relativo: 41.560738\n",
      "Vectores posición\n",
      "ángulo pred: 155.90785 ángulo gt: 86.6216 ángulo relativo: 23.260653\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(134.4625) ángulo gt: 131.72368 ángulo relativo: 104.9535\n",
      "Vectores posición\n",
      "ángulo pred: 11.909793 ángulo gt: 83.9349 ángulo relativo: 27.656748\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(136.6780) ángulo gt: 135.39696 ángulo relativo: 92.70928\n",
      "Vectores posición\n",
      "ángulo pred: 147.34413 ángulo gt: 121.0077 ángulo relativo: 59.354458\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(145.1426) ángulo gt: 144.33345 ángulo relativo: 71.52129\n",
      "Vectores posición\n",
      "ángulo pred: 121.33027 ángulo gt: 80.28568 ángulo relativo: 31.799683\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(158.5548) ángulo gt: 156.29187 ángulo relativo: 48.87044\n",
      "Vectores posición\n",
      "ángulo pred: 96.421486 ángulo gt: 112.90394 ángulo relativo: 26.28793\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(53.0499) ángulo gt: 64.37077 ángulo relativo: 43.518734\n",
      "Vectores posición\n",
      "ángulo pred: 62.31502 ángulo gt: 32.597675 ángulo relativo: 11.690964\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(61.7138) ángulo gt: 69.86122 ángulo relativo: 77.149445\n",
      "Vectores posición\n",
      "ángulo pred: 55.723206 ángulo gt: 123.78458 ángulo relativo: 76.08711\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(123.4374) ángulo gt: 119.28914 ángulo relativo: 118.322014\n",
      "Vectores posición\n",
      "ángulo pred: 6.756389 ángulo gt: 92.54784 ángulo relativo: 56.95118\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(98.9699) ángulo gt: 96.56856 ángulo relativo: 165.19853\n",
      "Vectores posición\n",
      "ángulo pred: 49.496742 ángulo gt: 91.77575 ángulo relativo: 81.86453\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(68.9947) ángulo gt: 66.276474 ángulo relativo: 117.78274\n",
      "Vectores posición\n",
      "ángulo pred: 32.586758 ángulo gt: 88.33997 ángulo relativo: 51.514748\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(142.5802) ángulo gt: 143.52945 ángulo relativo: 76.6601\n",
      "Vectores posición\n",
      "ángulo pred: 30.248253 ángulo gt: 86.30668 ángulo relativo: 48.8449\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(129.3673) ángulo gt: 124.723724 ángulo relativo: 106.49625\n",
      "Vectores posición\n",
      "ángulo pred: 114.6703 ángulo gt: 75.07191 ángulo relativo: 53.124084\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(74.9669) ángulo gt: 78.60705 ángulo relativo: 153.36314\n",
      "Vectores posición\n",
      "ángulo pred: 86.82054 ángulo gt: 132.35852 ángulo relativo: 74.046\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(135.7921) ángulo gt: 138.76436 ángulo relativo: 86.54777\n",
      "Vectores posición\n",
      "ángulo pred: 44.15459 ángulo gt: 89.63131 ángulo relativo: 44.48307\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(65.5718) ángulo gt: 69.14385 ángulo relativo: 130.43852\n",
      "Vectores posición\n",
      "ángulo pred: 133.05956 ángulo gt: 97.1315 ángulo relativo: 61.067654\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(47.1673) ángulo gt: 37.256065 ángulo relativo: 52.323254\n",
      "Vectores posición\n",
      "ángulo pred: 142.62764 ángulo gt: 103.39641 ángulo relativo: 49.72881\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(149.9008) ángulo gt: 149.82852 ángulo relativo: 61.50709\n",
      "Vectores posición\n",
      "ángulo pred: 144.88734 ángulo gt: 78.83292 ángulo relativo: 31.901745\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(68.2623) ángulo gt: 65.15109 ángulo relativo: 133.03207\n",
      "Vectores posición\n",
      "ángulo pred: 42.989643 ángulo gt: 123.531395 ángulo relativo: 64.262436\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(32.0814) ángulo gt: 33.71515 ángulo relativo: 19.99175\n",
      "Vectores posición\n",
      "ángulo pred: 74.7564 ángulo gt: 124.92176 ángulo relativo: 20.350231\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(143.0789) ángulo gt: 143.97101 ángulo relativo: 73.264305\n",
      "Vectores posición\n",
      "ángulo pred: 47.99557 ángulo gt: 106.23804 ángulo relativo: 37.3614\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(156.3495) ángulo gt: 154.47757 ángulo relativo: 49.37158\n",
      "Vectores posición\n",
      "ángulo pred: 93.597946 ángulo gt: 35.374405 ángulo relativo: 24.176262\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(50.1886) ángulo gt: 59.38856 ángulo relativo: 20.262472\n",
      "Vectores posición\n",
      "ángulo pred: 82.34618 ángulo gt: 34.682625 ángulo relativo: 18.367706\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(133.2110) ángulo gt: 135.03688 ángulo relativo: 97.1209\n",
      "Vectores posición\n",
      "ángulo pred: 95.22567 ángulo gt: 123.10848 ángulo relativo: 37.075134\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(178.2941) ángulo gt: 178.8567 ángulo relativo: 6.233537\n",
      "Vectores posición\n",
      "ángulo pred: 22.222754 ángulo gt: 62.961693 ángulo relativo: 3.1211088\n",
      "---\n",
      "Matrices Rotación\n",
      "ángulo pred: tensor(27.9846) ángulo gt: 26.83077 ángulo relativo: 53.278996\n",
      "Vectores posición\n",
      "ángulo pred: 89.551216 ángulo gt: 125.9691 ángulo relativo: 37.82213\n",
      "---\n",
      "RRA 0.9666666666666667 RTA 0.06666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1284189/861320804.py:25: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  relativeRotationAccuracy = np.arccos((torch.trace(rotationMat0@rotationMat1.T) - 1)/2)\n",
      "/tmp/ipykernel_1284189/861320804.py:38: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  thing1 = np.arccos((np.dot(a,rotationMat0[:, 0])/(np.linalg.norm(a)*np.linalg.norm(rotationMat0[:, 0]))))\n",
      "/tmp/ipykernel_1284189/861320804.py:47: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  rta = np.clip(1 - np.dot(vectFrom0to1, gtVectFrom0to1)**2, a_min=1e-15, a_max = 1)\n",
      "/tmp/ipykernel_1284189/861320804.py:50: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  loopRRA = np.rad2deg(relativeRotationAccuracy) - np.rad2deg(r)\n",
      "/tmp/ipykernel_1284189/861320804.py:53: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  if np.abs(loopRRA) < 15:\n",
      "/tmp/ipykernel_1284189/861320804.py:60: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  print(\"ángulo pred:\", np.rad2deg(relativeRotationAccuracy), \"ángulo gt:\", np.rad2deg(r), \"ángulo relativo:\", np.rad2deg(np.arccos((np.trace((relativePose[:3, :3])@(relativePoseGt[:3, :3].T)) -1)/2)))\n"
     ]
    }
   ],
   "source": [
    "mRTA = 0\n",
    "mRRA = 0\n",
    "for i in range(1, len(poses)):\n",
    "    homoMat0 = poses[i-1].cpu().detach()\n",
    "    homoMat1 = poses[i].cpu().detach()\n",
    "    \n",
    "    \n",
    "    rotationMat0 = homoMat0[:3, :3]\n",
    "    transVect0 = homoMat0[:, 3]\n",
    "\n",
    "    rotationMat1 = homoMat1[:3, :3]\n",
    "    transVect1 = homoMat1[:, 3]\n",
    "\n",
    "\n",
    "    gtHomoMat0 = transformationMatrices[i-1].numpy()\n",
    "    gtHomoMat1 = transformationMatrices[i].numpy()\n",
    "    \n",
    "\n",
    "    gtRotationMat0 = gtHomoMat0[:3, :3]\n",
    "    gtTransVect0 = gtHomoMat0[:, 3]\n",
    "    \n",
    "    gtRotationMat1 = gtHomoMat1[:3, :3]\n",
    "    gtTransVect1 = gtHomoMat1[:, 3]\n",
    "\n",
    "    relativeRotationAccuracy = np.arccos((torch.trace(rotationMat0@rotationMat1.T) - 1)/2)\n",
    "\n",
    "    r = np.arccos((np.trace(gtRotationMat0@gtRotationMat1.T) - 1)/2)\n",
    "    \n",
    "\n",
    "    relativePose = np.linalg.inv(homoMat0)@homoMat1\n",
    "    relativePoseGt = np.linalg.inv(gtHomoMat0)@gtHomoMat1\n",
    "\n",
    "\n",
    "\n",
    "    a = (np.linalg.inv(homoMat0)@homoMat1[:, 3])[:3]\n",
    "    b = (np.linalg.inv(gtHomoMat0)@gtHomoMat1[:, 3])[:3]\n",
    "\n",
    "    thing1 = np.arccos((np.dot(a,rotationMat0[:, 0])/(np.linalg.norm(a)*np.linalg.norm(rotationMat0[:, 0]))))\n",
    "    thing2 = np.arccos(np.dot(b,gtRotationMat0[:, 0])/(np.linalg.norm(b)*np.linalg.norm(gtRotationMat0[:, 0])))\n",
    "\n",
    "    vectFrom0to1 = relativePose[:3, 3]\n",
    "    gtVectFrom0to1 = relativePoseGt[:3, 3]\n",
    "\n",
    "    vectFrom0to1 = vectFrom0to1 / (np.linalg.norm(vectFrom0to1) + 1e-15) \n",
    "    gtVectFrom0to1 = gtVectFrom0to1 / (np.linalg.norm(gtVectFrom0to1) + 1e-15)\n",
    "\n",
    "    rta = np.clip(1 - np.dot(vectFrom0to1, gtVectFrom0to1)**2, a_min=1e-15, a_max = 1)\n",
    "    rta = np.arccos(np.sqrt(1-rta))\n",
    "\n",
    "    loopRRA = np.rad2deg(relativeRotationAccuracy) - np.rad2deg(r)\n",
    "    loopRTA = np.rad2deg(thing1) - np.rad2deg(thing2)\n",
    "\n",
    "    if np.abs(loopRRA) < 15:\n",
    "        mRRA += 1\n",
    "\n",
    "    if np.rad2deg(rta) <15:\n",
    "        mRTA +=1 \n",
    "\n",
    "    print(\"Matrices Rotación\")\n",
    "    print(\"ángulo pred:\", np.rad2deg(relativeRotationAccuracy), \"ángulo gt:\", np.rad2deg(r), \"ángulo relativo:\", np.rad2deg(np.arccos((np.trace((relativePose[:3, :3])@(relativePoseGt[:3, :3].T)) -1)/2)))\n",
    "    print(\"Vectores posición\")\n",
    "    print(\"ángulo pred:\", np.rad2deg(thing1), \"ángulo gt:\", np.rad2deg(thing2), \"ángulo relativo:\", np.rad2deg(rta))\n",
    "    print('---')\n",
    "\n",
    "print(\"RRA\", mRRA/len(poses), \"RTA\", mRTA/len(poses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3b3a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf25e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dStuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
